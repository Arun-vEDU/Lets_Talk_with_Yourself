{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. Prompt\n",
    "from langchain import PromptTemplate\n",
    "\n",
    "personal_prompt_template = \"\"\"\n",
    "    I'm your friendly AI assistant, here to provide information about my background, education, work experience, and beliefs. \n",
    "    Feel free to ask me any questions about myself, and I'll do my best to provide accurate and helpful answers.\n",
    "    \n",
    "    Context: {context}\n",
    "    Question: {question}\n",
    "    Answer:\n",
    "    \"\"\".strip()\n",
    "\n",
    "PERSONAL_PROMPT = PromptTemplate.from_template(template=personal_prompt_template)\n",
    "PERSONAL_PROMPT\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz  # PyMuPDF\n",
    "from unstructured.partition.md import partition_md\n",
    "\n",
    "def load_pdf(file_path):\n",
    "    doc = fitz.open(file_path)\n",
    "    text = \"\"\n",
    "    for page in doc:\n",
    "        text += page.get_text()\n",
    "    return text\n",
    "\n",
    "# Load your resume\n",
    "resume_text = load_pdf(\"resume.pdf\")\n",
    "\n",
    "# Load LinkedIn profile\n",
    "linkedin_text = load_pdf(\"linkedin_profile.pdf\")\n",
    "\n",
    "def load_markdown(file_path):\n",
    "    \"\"\"\n",
    "    Load text from a markdown file using unstructured.\n",
    "    \"\"\"\n",
    "    elements = partition_md(filename=file_path)\n",
    "    return \"\\n\".join([str(el) for el in elements])\n",
    "\n",
    "# Load your personal blog (if applicable)\n",
    "blog_text = load_markdown(\"personal_blog.md\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = [\n",
    "    {\"content\": resume_text, \"source\": \"resume.pdf\"},\n",
    "    {\"content\": linkedin_text, \"source\": \"linkedin_profile.pdf\"},\n",
    "    {\"content\": blog_text, \"source\": \"personal_blog.md\"},  \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=700,\n",
    "    chunk_overlap=100\n",
    ")\n",
    "\n",
    "doc_chunks = []\n",
    "for doc in documents:\n",
    "    chunks = text_splitter.split_text(doc[\"content\"])\n",
    "    for chunk in chunks:\n",
    "        doc_chunks.append({\"content\": chunk, \"source\": doc[\"source\"]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_chunks[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "# Initialize Sentence Transformers embeddings\n",
    "embedding_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "print(\"Embedding model initialized successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the embedding model\n",
    "text = \"This is a test sentence.\"\n",
    "embeddings = embedding_model.embed_query(text)\n",
    "print(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import FAISS\n",
    "from langchain.schema import Document\n",
    "\n",
    "# Convert chunks to LangChain Document format\n",
    "docs = [Document(page_content=chunk[\"content\"], metadata={\"source\": chunk[\"source\"]}) for chunk in doc_chunks]\n",
    "\n",
    "# Create vector store\n",
    "vector_store = FAISS.from_documents(docs, embedding_model)\n",
    "\n",
    "# Save the vector store locally\n",
    "vector_store.save_local(\"personal_vector_store\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline\n",
    "from langchain import HuggingFacePipeline\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "# Load the tokenizer and model\n",
    "model_id = \"fastchat-t5-3b-v1.0\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/flan-t5-large\")  # Use a compatible tokenizer\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_id)\n",
    "\n",
    "# Create a text generation pipeline\n",
    "pipe = pipeline(\n",
    "    task=\"text2text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    max_new_tokens=200,  # Limit response length\n",
    "    model_kwargs={\n",
    "        \"temperature\": 0.7,\n",
    "        \"repetition_penalty\": 1.2\n",
    "    }\n",
    ")\n",
    "\n",
    "# Wrap the pipeline in LangChain's HuggingFacePipeline\n",
    "llm = HuggingFacePipeline(pipeline=pipe)\n",
    "\n",
    "# Create a RetrievalQA chain\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=vector_store.as_retriever(search_kwargs={\"k\": 3})  # Retrieve top 3 chunks\n",
    ")\n",
    "\n",
    "# Clean and ask a question\n",
    "def clean_text(text):\n",
    "    return text.strip().replace(\"\\n\", \" \").replace(\"\\r\", \" \")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ask a question\n",
    "question = \"How old is Arunya P. Senadeera?\"\n",
    "try:\n",
    "    response = qa_chain.run(question)\n",
    "    print(f\"Question: {question}\")\n",
    "    print(f\"Answer: {response}\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "# Step 6: Initialize Memory for Conversation History\n",
    "memory = ConversationBufferMemory(\n",
    "    memory_key=\"chat_history\",  # Key to store chat history\n",
    "    return_messages=True  # Return chat history as a list of messages\n",
    ")\n",
    "\n",
    "# Step 7: Create the Conversational Retrieval Chain\n",
    "chain = ConversationalRetrievalChain.from_llm(\n",
    "    llm=llm,  # Language model (HuggingFacePipeline)\n",
    "    retriever=vector_store.as_retriever(search_kwargs={\"k\": 3}),  # Retrieve top 3 chunks\n",
    "    memory=memory  # Add memory for conversation history\n",
    ")\n",
    "\n",
    "# Test the chatbot\n",
    "query = \"What is my highest level of education?\"\n",
    "response = chain({\"question\": query})\n",
    "print(response[\"answer\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
