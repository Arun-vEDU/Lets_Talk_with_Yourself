{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['context', 'question'], input_types={}, partial_variables={}, template=\"I'm your friendly AI assistant, here to provide information about my background, education, work experience, and beliefs. \\n    Feel free to ask me any questions about myself, and I'll do my best to provide accurate and helpful answers.\\n    \\n    Context: {context}\\n    Question: {question}\\n    Answer:\")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#1. Prompt\n",
    "from langchain import PromptTemplate\n",
    "\n",
    "personal_prompt_template = \"\"\"\n",
    "    I'm your friendly AI assistant, here to provide information about my background, education, work experience, and beliefs. \n",
    "    Feel free to ask me any questions about myself, and I'll do my best to provide accurate and helpful answers.\n",
    "    \n",
    "    Context: {context}\n",
    "    Question: {question}\n",
    "    Answer:\n",
    "    \"\"\".strip()\n",
    "\n",
    "PERSONAL_PROMPT = PromptTemplate.from_template(template=personal_prompt_template)\n",
    "PERSONAL_PROMPT\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz  # PyMuPDF\n",
    "from unstructured.partition.md import partition_md\n",
    "\n",
    "def load_pdf(file_path):\n",
    "    doc = fitz.open(file_path)\n",
    "    text = \"\"\n",
    "    for page in doc:\n",
    "        text += page.get_text()\n",
    "    return text\n",
    "\n",
    "# Load your resume\n",
    "resume_text = load_pdf(\"resume.pdf\")\n",
    "\n",
    "# Load LinkedIn profile\n",
    "linkedin_text = load_pdf(\"linkedin_profile.pdf\")\n",
    "\n",
    "def load_markdown(file_path):\n",
    "    \"\"\"\n",
    "    Load text from a markdown file using unstructured.\n",
    "    \"\"\"\n",
    "    elements = partition_md(filename=file_path)\n",
    "    return \"\\n\".join([str(el) for el in elements])\n",
    "\n",
    "# Load your personal blog (if applicable)\n",
    "blog_text = load_markdown(\"personal_blog.md\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = [\n",
    "    {\"content\": resume_text, \"source\": \"resume.pdf\"},\n",
    "    {\"content\": linkedin_text, \"source\": \"linkedin_profile.pdf\"},\n",
    "    {\"content\": blog_text, \"source\": \"personal_blog.md\"},  \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=700,\n",
    "    chunk_overlap=100\n",
    ")\n",
    "\n",
    "doc_chunks = []\n",
    "for doc in documents:\n",
    "    chunks = text_splitter.split_text(doc[\"content\"])\n",
    "    for chunk in chunks:\n",
    "        doc_chunks.append({\"content\": chunk, \"source\": doc[\"source\"]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'content': 'Asian Institute of Technology \\nBangkok, Thailand \\nM. Eng. in Mechatronics \\n2015 - 2017 \\n• \\nThesis: “Sensorless Terrain Estimation  and Longitudinal Acceleration Suppression for a Wheeled Mobile \\nRobot”,[PDF] \\n• \\nSupervisor: Dr. A. M. Harsha S. Abeykoon \\n• \\nAwards: AIT Fellowship \\n \\nAsian Institute of Technology \\nBangkok, Thailand \\nB.Sc. in Engineering - Mechatronics \\n2009 - 2013 \\n• \\nThesis: “Motion Detection and Target tracking using a Pan-Tilt Camera” \\n• \\nSupervisor: Prof. Manukid Pranichkun \\n• \\nRank: Second Class-Upper Division \\n \\nEmployment \\nUniversity of Moratuwa, Department of Electrical Engineering \\nKatubedda, Sri Lanka \\nLecture (Career break from AIT, Thailand)',\n",
       " 'source': 'resume.pdf'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_chunks[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Arunya Senadeera\\AppData\\Local\\Temp\\ipykernel_20368\\3725019889.py:4: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embedding_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding model initialized successfully!\n"
     ]
    }
   ],
   "source": [
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "# Initialize Sentence Transformers embeddings\n",
    "embedding_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "print(\"Embedding model initialized successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.08429645001888275, 0.05795370042324066, 0.004493384622037411, 0.10582107305526733, 0.00708338338881731, -0.01784462481737137, -0.01688799262046814, -0.015228294767439365, 0.040473103523254395, 0.033422552049160004, 0.10432764142751694, -0.04703591763973236, 0.006884727627038956, 0.04101794213056564, 0.018711984157562256, -0.04149234667420387, 0.023647490888834, -0.056501924991607666, -0.033696211874485016, 0.050990939140319824, 0.06930320709943771, 0.05478423833847046, -0.00978838186711073, 0.02369716763496399, 0.019996603950858116, 0.009717307053506374, -0.058899134397506714, 0.007307387888431549, 0.047026533633470535, -0.004510192666202784, -0.055799663066864014, -0.004159401170909405, 0.06475706398487091, 0.04807629808783531, 0.01702086813747883, -0.0031833983957767487, 0.05740240216255188, 0.035231851041316986, -0.0058838739059865475, 0.014832890592515469, 0.011576258577406406, -0.10748074948787689, 0.019104115664958954, 0.022085731849074364, 0.010864544659852982, 0.0037819670978933573, -0.03194031864404678, 0.010727879591286182, -0.004842301364988089, -0.028336284682154655, -0.052573543041944504, -0.07058683782815933, -0.05755584314465523, -0.01363292708992958, 0.005682237446308136, 0.023074662312865257, 0.03569784387946129, 0.01499833445996046, 0.049742717295885086, 0.042628273367881775, -0.03458888828754425, -0.02436010167002678, -0.07152242958545685, 0.08312481641769409, 0.1489190012216568, 0.05401795357465744, -0.04133826866745949, -0.08989853411912918, -0.04452534019947052, 0.014739089645445347, 0.02231181412935257, 0.019573654979467392, 0.043633896857500076, 0.009812303818762302, -0.0038072660099714994, -0.014992906711995602, -0.0006992021226324141, -0.11284737288951874, 0.12384185940027237, 0.05555810034275055, -0.08891021460294724, -0.07885005325078964, -0.016893295571208, 0.04597761109471321, -0.008727028965950012, -0.05813834071159363, 0.07659002393484116, -0.03432244807481766, -0.07924245297908783, 0.02176620252430439, -0.034249622374773026, -0.026446698233485222, 0.00529809296131134, 0.004527391865849495, -0.05101802572607994, -0.016586285084486008, -0.083045095205307, -0.00237126718275249, -0.0011132898507639766, 0.043059468269348145, 0.00437790947034955, 0.025499016046524048, 0.007468856871128082, 0.06316963583230972, -0.09805838018655777, -0.08793121576309204, 0.00035324913915246725, -0.05766992270946503, 0.02063729055225849, -0.007735349703580141, -0.030646445229649544, -0.0020990492776036263, 0.05955920368432999, 0.01957971788942814, -0.012472603470087051, 0.009708081372082233, -0.12317747622728348, 0.025990866124629974, -0.01577666774392128, 0.04943500831723213, 0.06154121831059456, 0.07830115407705307, -0.029691405594348907, -0.01318654790520668, -0.062351081520318985, -0.08068260550498962, 0.05496548116207123, -6.962592629874569e-33, -0.02584214322268963, -0.0667799636721611, 0.043513912707567215, 0.07399919629096985, 0.010259317234158516, -0.024453626945614815, -0.02124880999326706, 0.06839413195848465, -0.022924913093447685, -7.918568007880822e-05, -0.002582322806119919, -0.09488376975059509, 0.013310965150594711, 0.04063050076365471, 0.08562172204256058, 0.09818190336227417, -0.07668068259954453, 0.0695323720574379, -0.046746306121349335, 0.05553460866212845, -0.03531915321946144, 0.03812241181731224, -0.018466759473085403, -0.06546914577484131, -0.09128416329622269, -0.11191494762897491, 0.002208059187978506, 0.008413429372012615, -0.047066520899534225, 0.020357005298137665, 0.010655523277819157, 0.026104306802153587, -0.02678094618022442, 0.060119349509477615, 0.020277541130781174, 0.016695203259587288, 0.035287950187921524, -0.07817293703556061, -0.025837019085884094, 0.010249676182866096, -0.061475448310375214, -0.028475841507315636, -0.010275875218212605, 0.012673887424170971, 0.09546958655118942, -0.012143969535827637, -0.014245756901800632, -0.026192422956228256, -0.006299289874732494, 0.022196702659130096, -0.026083998382091522, 0.043943438678979874, 0.07364547997713089, -0.033389318734407425, 0.03217765688896179, 0.06466920673847198, 0.049319397658109665, -0.010532105341553688, -0.034727953374385834, 0.06571027636528015, -0.02722320333123207, 0.06035550311207771, -0.06002897396683693, 0.056270819157361984, 0.006808890961110592, 0.018710684031248093, -0.04290997236967087, -0.040959738194942474, 0.052991971373558044, 0.03309451788663864, -0.015546540729701519, -0.0729866772890091, -0.05088840797543526, 0.06311015039682388, -0.012977268546819687, -0.07079785317182541, 0.014870642684400082, 0.03425684943795204, 0.008294953033328056, 0.009790817275643349, 0.023416811600327492, -0.1121094673871994, 0.028235893696546555, -0.05278497934341431, -0.09382616728544235, -0.0005618229042738676, -0.0193162988871336, -0.08856188505887985, 0.024456294253468513, -0.024815479293465614, 0.012973355129361153, 0.023209165781736374, 0.03923258185386658, -0.03373201936483383, 0.02211729809641838, 2.8970638676350994e-33, -0.0342555046081543, 0.06554907560348511, -0.07177574187517166, 0.08654047548770905, 0.08716150373220444, -0.030142076313495636, 0.06583678722381592, -0.033409927040338516, -0.03749044984579086, 0.13489805161952972, -0.04510383680462837, 0.03301926702260971, -0.004240371286869049, -0.022089483216404915, 0.0393681563436985, 0.011286066845059395, 0.010816551744937897, -0.04902677983045578, -0.01828550361096859, -0.023917129263281822, -0.058630574494600296, 0.12643465399742126, 0.017078369855880737, 0.10173558443784714, -0.0416656956076622, 0.0037376838736236095, 0.014300912618637085, -0.0677729994058609, -0.07409573346376419, 0.02024652063846588, 0.021144971251487732, -0.03383060544729233, -0.09985555708408356, -0.008472098968923092, 0.010729043744504452, 0.008691216818988323, 0.10240786522626877, -0.07643360644578934, -0.051760025322437286, 0.030973203480243683, 0.004117793403565884, 0.02747085876762867, 0.06673243641853333, 0.14349904656410217, 0.013602414168417454, 0.012040949426591396, -0.02445257268846035, -0.14091476798057556, -0.04332244023680687, 0.0430765226483345, -0.055588796734809875, 0.02399272285401821, -0.012689615599811077, 0.01733713038265705, -0.06776981800794601, 0.011358002200722694, -0.03187907487154007, -0.029640544205904007, -0.02505963295698166, -0.0022673620842397213, -0.09762755036354065, 0.06035014986991882, 0.006841632537543774, 0.032143980264663696, 0.0229487381875515, -0.07136144489049911, -0.07719403505325317, 0.09534019231796265, 0.026547832414507866, -0.06032518297433853, 0.1029110699892044, 0.029590923339128494, -0.1088823452591896, -0.024806585162878036, -0.03323342651128769, -0.08235324919223785, -0.08177339285612106, -0.017642265185713768, -0.019313713535666466, -0.08298187702894211, -0.018056092783808708, -0.05327996611595154, 0.0046994793228805065, 0.015258055180311203, -0.08144969493150711, 0.03853044658899307, -0.0044259740971028805, -0.027970891445875168, -0.01020356360822916, 0.044200025498867035, -0.016933942213654518, -0.05513205751776695, 0.039740558713674545, -0.02304839715361595, -0.026844915002584457, -1.9580529198037766e-08, -0.018671976402401924, -0.011711076833307743, 0.014647992327809334, 0.006658591330051422, 0.000737135938834399, 0.06258220225572586, 0.06776583939790726, -0.00595501996576786, -0.03051966056227684, -0.0008199234143830836, 0.06429600715637207, 0.06641694158315659, -0.09795115888118744, -8.581262591178529e-06, 0.06032385304570198, -0.02740226686000824, 0.024251503869891167, -0.021256377920508385, -0.002669567707926035, 0.08429724723100662, -0.048472147434949875, 0.04775244742631912, 0.013190969824790955, 0.04648691043257713, -0.0029792862478643656, 0.04415562376379967, 0.0995890274643898, 0.06427270919084549, 0.0033607103396207094, 0.044975586235523224, 0.11025182902812958, 0.06426689773797989, -0.03681520000100136, -0.0315479077398777, -0.014766705222427845, 0.08226466178894043, 0.056715019047260284, -0.06494801491498947, 0.02308064140379429, 0.01923477277159691, -0.05154437944293022, 0.07222788780927658, -0.014392110519111156, 0.07818154990673065, -0.011162733659148216, -0.04956832528114319, -0.03159083425998688, -0.02954753488302231, -0.015165088698267937, -0.053925566375255585, -0.008859294466674328, 0.021064816042780876, 0.04502767324447632, -0.022881215438246727, 0.026812829077243805, -0.033065542578697205, -0.003495130455121398, -0.030414894223213196, -0.07156670838594437, 0.02329576201736927, 0.08976897597312927, 0.004571141675114632, 0.08188030123710632, -0.09904709458351135]\n"
     ]
    }
   ],
   "source": [
    "# Test the embedding model\n",
    "text = \"This is a test sentence.\"\n",
    "embeddings = embedding_model.embed_query(text)\n",
    "print(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import FAISS\n",
    "from langchain.schema import Document\n",
    "\n",
    "# Convert chunks to LangChain Document format\n",
    "docs = [Document(page_content=chunk[\"content\"], metadata={\"source\": chunk[\"source\"]}) for chunk in doc_chunks]\n",
    "\n",
    "# Create vector store\n",
    "vector_store = FAISS.from_documents(docs, embedding_model)\n",
    "\n",
    "# Save the vector store locally\n",
    "vector_store.save_local(\"personal_vector_store\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n",
      "C:\\Users\\Arunya Senadeera\\AppData\\Local\\Temp\\ipykernel_20368\\2736169585.py:23: LangChainDeprecationWarning: The class `HuggingFacePipeline` was deprecated in LangChain 0.0.37 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFacePipeline``.\n",
      "  llm = HuggingFacePipeline(pipeline=pipe)\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline\n",
    "from langchain import HuggingFacePipeline\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "# Load the tokenizer and model\n",
    "model_id = \"fastchat-t5-3b-v1.0\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/flan-t5-large\")  # Use a compatible tokenizer\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_id)\n",
    "\n",
    "# Create a text generation pipeline\n",
    "pipe = pipeline(\n",
    "    task=\"text2text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    max_new_tokens=200,  # Limit response length\n",
    "    model_kwargs={\n",
    "        \"temperature\": 0.7,\n",
    "        \"repetition_penalty\": 1.2\n",
    "    }\n",
    ")\n",
    "\n",
    "# Wrap the pipeline in LangChain's HuggingFacePipeline\n",
    "llm = HuggingFacePipeline(pipeline=pipe)\n",
    "\n",
    "# Create a RetrievalQA chain\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=vector_store.as_retriever(search_kwargs={\"k\": 3})  # Retrieve top 3 chunks\n",
    ")\n",
    "\n",
    "# Clean and ask a question\n",
    "def clean_text(text):\n",
    "    return text.strip().replace(\"\\n\", \" \").replace(\"\\r\", \" \")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Arunya Senadeera\\AppData\\Local\\Temp\\ipykernel_20368\\3657548938.py:5: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  memory = ConversationBufferMemory(\n",
      "C:\\Users\\Arunya Senadeera\\AppData\\Local\\Temp\\ipykernel_20368\\3657548938.py:19: LangChainDeprecationWarning: The method `Chain.__call__` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  response = chain({\"question\": query})\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (584 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: Arunya P. Senadeera is 30 years old.\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "# Step 6: Initialize Memory for Conversation History\n",
    "memory = ConversationBufferMemory(\n",
    "    memory_key=\"chat_history\",  # Key to store chat history\n",
    "    return_messages=True  # Return chat history as a list of messages\n",
    ")\n",
    "\n",
    "# Step 7: Create the Conversational Retrieval Chain\n",
    "chain = ConversationalRetrievalChain.from_llm(\n",
    "    llm=llm,  # Language model (HuggingFacePipeline)\n",
    "    retriever=vector_store.as_retriever(search_kwargs={\"k\": 3}),  # Retrieve top 3 chunks\n",
    "    memory=memory  # Add memory for conversation history\n",
    ")\n",
    "\n",
    "# Test the chatbot\n",
    "query = \"How old is Arunya P. Senadeera?\"\n",
    "response = chain({\"question\": query})\n",
    "print(response[\"answer\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the model, save the model, tokenizer, and pipeline for dash application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Couldn't find program: 'False'\n"
     ]
    }
   ],
   "source": [
    "%%script False\n",
    "# Save the tokenizer and model\n",
    "model.save_pretrained(\"saved_model_HugingFace\")\n",
    "tokenizer.save_pretrained(\"saved_tokenizer_HugingFace\")\n",
    "\n",
    "# Save the pipeline\n",
    "import pickle\n",
    "with open(\"saved_pipeline_HugingFace.pkl\", \"wb\") as f:\n",
    "    pickle.dump(pipe, f) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "    {\n",
      "        \"question\": \"How old is Arunya?\",\n",
      "        \"answer\": \"Answer: Arunya P. Senadeera is 30 years old.\"\n",
      "    },\n",
      "    {\n",
      "        \"question\": \"What is your highest level of education?\",\n",
      "        \"answer\": \"Your highest level of education is Doctoral Student , Data Science & Artificial Intelligence\"\n",
      "    },\n",
      "    {\n",
      "        \"question\": \"What major or field of study did you pursue during your education?\",\n",
      "        \"answer\": \"Answer: Electronics and Computer Science\"\n",
      "    },\n",
      "    {\n",
      "        \"question\": \"How many years of work experience do you have?\",\n",
      "        \"answer\": \"7 years 8 months\"\n",
      "    },\n",
      "    {\n",
      "        \"question\": \"What type of work or industry have you been involved in?\",\n",
      "        \"answer\": \"As an academic researcher\"\n",
      "    },\n",
      "    {\n",
      "        \"question\": \"Can you describe your current role or job responsibilities?\",\n",
      "        \"answer\": \"As a Senior Research Associate at the Asian Institute of Technology, my current role is to develop and manage the Advanced Telecommunication Laboratories and provide curriculum development and supervision for the IoT systems engineering postgraduate program. I also provide academic writing guidance and feedback for 1st year postgraduate students and develop research proposals for seed funding representing the AIT-ICT program.\"\n",
      "    },\n",
      "    {\n",
      "        \"question\": \"What are your core beliefs regarding the role of technology in shaping society?\",\n",
      "        \"answer\": \"My core beliefs regarding the role of technology in shaping society are that it has the potential to create a meaningful impact on society and enhance the quality of life for people.\"\n",
      "    },\n",
      "    {\n",
      "        \"question\": \"How do you think cultural values should influence technological advancements?\",\n",
      "        \"answer\": \"I don't know\"\n",
      "    },\n",
      "    {\n",
      "        \"question\": \"As a student, what is the most challenging aspect of your studies so far?\",\n",
      "        \"answer\": \"The most challenging aspect of my studies so far as a student is developing a research proposal for seed funding\"\n",
      "    },\n",
      "    {\n",
      "        \"question\": \"What specific research interests or academic goals do you hope to achieve during your time as a student?\",\n",
      "        \"answer\": \"I hope to achieve academic excellence and to contribute to the field of Assistive Robotics and IoT by identifying and addressing gaps in practical research applications and by leveraging these technologies to create a meaningful impact on society and enhance the quality of life for people.\"\n",
      "    }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# --------------------- Questions to Ask ---------------------\n",
    "questions = [\n",
    "    \"How old is Arunya?\",\n",
    "    \"What is your highest level of education?\",\n",
    "    \"What major or field of study did you pursue during your education?\",\n",
    "    \"How many years of work experience do you have?\",\n",
    "    \"What type of work or industry have you been involved in?\",\n",
    "    \"Can you describe your current role or job responsibilities?\",\n",
    "    \"What are your core beliefs regarding the role of technology in shaping society?\",\n",
    "    \"How do you think cultural values should influence technological advancements?\",\n",
    "    \"As a student, what is the most challenging aspect of your studies so far?\",\n",
    "    \"What specific research interests or academic goals do you hope to achieve during your time as a student?\"\n",
    "]\n",
    "\n",
    "# --------------------- Ask Questions and Store Results ---------------------\n",
    "results = []\n",
    "\n",
    "for question in questions:\n",
    "    response = chain({\"question\": question})\n",
    "    answer = response.get(\"answer\", \"No answer found.\")\n",
    "    results.append({\n",
    "        \"question\": question,\n",
    "        \"answer\": answer\n",
    "    })\n",
    "\n",
    "# --------------------- Print Results in JSON Format ---------------------\n",
    "print(json.dumps(results, indent=4))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
